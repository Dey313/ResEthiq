{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPnZs6JuWd3gAjeHwzPJviF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dey313/ResEthiq/blob/main/F_AI_(updated).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install pandas numpy scipy scikit-learn reportlab matplotlib"
      ],
      "metadata": {
        "id": "Mmo_BiJumeYV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "csv_path = next(iter(uploaded.keys()))\n",
        "csv_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "AkKjrb7ToCOa",
        "outputId": "913da5b9-e084-43c9-924a-f10245e5f0b0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b26b5ebb-8ba0-449d-8436-b5514febda38\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b26b5ebb-8ba0-449d-8436-b5514febda38\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 2025_06_09_morphometric data.xlsx to 2025_06_09_morphometric data (1).xlsx\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2025_06_09_morphometric data (1).xlsx'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io, re, math, json, hashlib\n",
        "from dataclasses import dataclass, asdict\n",
        "from datetime import datetime\n",
        "from typing import Any, Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import chisquare, ks_2samp, skew, kurtosis\n",
        "from scipy.stats import entropy as scipy_entropy\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "from reportlab.lib.pagesizes import A4\n",
        "from reportlab.lib.units import cm\n",
        "from reportlab.lib import colors\n",
        "from reportlab.pdfgen import canvas\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 0) Robust CSV loader (fixes UnicodeDecodeError)\n",
        "# =========================================================\n",
        "def load_csv_robust(path: str, nrows: int | None = None) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    encodings = [\"utf-8\", \"utf-8-sig\", \"cp1252\", \"iso-8859-1\", \"latin1\", \"utf-16\", \"utf-16le\", \"utf-16be\"]\n",
        "    errors_mode = [\"strict\", \"replace\"]  # strict first; replace as fallback\n",
        "    delimiter_candidates = [\",\", \";\", \"\\t\", \"|\"]\n",
        "\n",
        "    last_err = None\n",
        "\n",
        "    for enc in encodings:\n",
        "        for err in errors_mode:\n",
        "            for sep in delimiter_candidates:\n",
        "                try:\n",
        "                    df = pd.read_csv(\n",
        "                        path,\n",
        "                        encoding=enc,\n",
        "                        encoding_errors=err,  # pandas >= 2.0\n",
        "                        sep=sep,\n",
        "                        engine=\"python\",      # tolerant parser\n",
        "                        nrows=nrows\n",
        "                    )\n",
        "                    # sanity check: \"real\" CSV usually has >=2 columns\n",
        "                    if df.shape[1] >= 2:\n",
        "                        return df, {\"encoding\": enc, \"encoding_errors\": err, \"sep\": sep}\n",
        "                except Exception as e:\n",
        "                    last_err = e\n",
        "\n",
        "    # If we got here, maybe it's not a CSV at all (e.g., Excel zip)\n",
        "    with open(path, \"rb\") as f:\n",
        "        sig = f.read(8)\n",
        "    raise RuntimeError(\n",
        "        f\"Could not parse as CSV with common encodings/separators. \"\n",
        "        f\"First 8 bytes: {sig}. Last error: {last_err}\"\n",
        "    )\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 1) Config\n",
        "# =========================================================\n",
        "HEALTHCARE_KEYWORDS = [\n",
        "    \"patient\", \"mrn\", \"ehr\", \"icd\", \"diagnosis\", \"diag\", \"drug\", \"med\", \"dose\",\n",
        "    \"lab\", \"test\", \"hba1c\", \"bp\", \"systolic\", \"diastolic\", \"spo2\", \"hr\", \"heart_rate\",\n",
        "    \"temperature\", \"temp\", \"admission\", \"discharge\", \"encounter\", \"visit\", \"hospital\", \"clinic\"\n",
        "]\n",
        "AI_KEYWORDS = [\n",
        "    \"prompt\", \"completion\", \"token\", \"logprob\", \"embedding\", \"model\", \"train\", \"eval\",\n",
        "    \"benchmark\", \"label\", \"annotation\", \"dataset\", \"loss\", \"accuracy\", \"f1\", \"roc\",\n",
        "    \"latency\", \"inference\", \"input\", \"output\"\n",
        "]\n",
        "\n",
        "MAX_ROWS_FOR_HEAVY = 300_000\n",
        "SAMPLE_ROWS = 120_000\n",
        "\n",
        "THRESH = {\n",
        "    \"dup_row_pct_warn\": 0.02,\n",
        "    \"dup_row_pct_fail\": 0.10,\n",
        "    \"missing_overall_warn\": 0.10,\n",
        "    \"missing_overall_fail\": 0.30,\n",
        "    \"high_corr_warn\": 10,\n",
        "    \"high_corr_fail\": 30,\n",
        "    \"benford_p_warn\": 0.05,\n",
        "    \"benford_p_fail\": 0.01,\n",
        "    \"rounding_integer_warn\": 0.85,\n",
        "    \"rounding_integer_fail\": 0.95,\n",
        "    \"entropy_low_warn\": 1.0,\n",
        "    \"entropy_low_fail\": 0.6,\n",
        "    \"isoforest_outlier_warn\": 0.08,\n",
        "    \"isoforest_outlier_fail\": 0.15,\n",
        "}\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 2) Utilities\n",
        "# =========================================================\n",
        "def sha256_bytes(b: bytes) -> str:\n",
        "    return hashlib.sha256(b).hexdigest()\n",
        "\n",
        "def stable_dataset_fingerprint(df: pd.DataFrame) -> str:\n",
        "    schema = \"|\".join([f\"{c}:{str(t)}\" for c, t in zip(df.columns, df.dtypes)])\n",
        "    sample = df.head(500).to_csv(index=False).encode(\"utf-8\", errors=\"ignore\")\n",
        "    return sha256_bytes(schema.encode(\"utf-8\", errors=\"ignore\") + b\"\\n\" + sample)\n",
        "\n",
        "def safe_sample(df: pd.DataFrame, max_rows: int = MAX_ROWS_FOR_HEAVY) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    meta = {\"sampled\": False, \"original_rows\": int(len(df)), \"used_rows\": int(len(df))}\n",
        "    if len(df) > max_rows:\n",
        "        df2 = df.sample(n=min(SAMPLE_ROWS, len(df)), random_state=42)\n",
        "        meta[\"sampled\"] = True\n",
        "        meta[\"used_rows\"] = int(len(df2))\n",
        "        return df2, meta\n",
        "    return df, meta\n",
        "\n",
        "def to_numeric_clean(series: pd.Series) -> pd.Series:\n",
        "    s = pd.to_numeric(series, errors=\"coerce\")\n",
        "    s = s.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "    return s\n",
        "\n",
        "def top_keywords_score(df: pd.DataFrame, keywords: List[str], max_cells: int = 50_000) -> int:\n",
        "    blob = \" \".join([str(c).lower() for c in df.columns])\n",
        "    obj_cols = df.select_dtypes(include=[\"object\", \"string\"]).columns.tolist()\n",
        "    taken = 0\n",
        "    for c in obj_cols[:10]:\n",
        "        col = df[c].astype(str).head(500)\n",
        "        for v in col:\n",
        "            blob += \" \" + str(v).lower()\n",
        "            taken += 1\n",
        "            if taken >= max_cells:\n",
        "                break\n",
        "        if taken >= max_cells:\n",
        "            break\n",
        "    score = 0\n",
        "    for kw in keywords:\n",
        "        if kw in blob:\n",
        "            score += 1\n",
        "    return score\n",
        "\n",
        "def severity_from_threshold(value: float, warn: float, fail: float, higher_is_worse: bool = True) -> str:\n",
        "    if higher_is_worse:\n",
        "        if value >= fail: return \"FAIL\"\n",
        "        if value >= warn: return \"WARN\"\n",
        "        return \"INFO\"\n",
        "    else:\n",
        "        if value <= fail: return \"FAIL\"\n",
        "        if value <= warn: return \"WARN\"\n",
        "        return \"INFO\"\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 3) Data structures\n",
        "# =========================================================\n",
        "@dataclass\n",
        "class Finding:\n",
        "    name: str\n",
        "    severity: str\n",
        "    metric: Dict[str, Any]\n",
        "    rationale: str\n",
        "\n",
        "@dataclass\n",
        "class Invariant:\n",
        "    name: str\n",
        "    status: str  # PASS/FAIL/UNKNOWN\n",
        "    details: Dict[str, Any]\n",
        "    rationale: str\n",
        "\n",
        "@dataclass\n",
        "class DomainRouting:\n",
        "    p_healthcare: float\n",
        "    p_ai: float\n",
        "    chosen: str\n",
        "    evidence: Dict[str, Any]\n",
        "\n",
        "@dataclass\n",
        "class AISignals:\n",
        "    applicable: bool\n",
        "    outlier_rate: float\n",
        "    centroid_cosine: float\n",
        "    notes: List[str]\n",
        "    details: Dict[str, Any]\n",
        "\n",
        "@dataclass\n",
        "class RealityStress:\n",
        "    applicable: bool\n",
        "    rri: float\n",
        "    collapse_curve: List[Tuple[float, float]]\n",
        "    assumption_load: List[str]\n",
        "    notes: List[str]\n",
        "    details: Dict[str, Any]\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 4) Router (Healthcare vs AI company)\n",
        "# =========================================================\n",
        "def domain_router(df: pd.DataFrame) -> DomainRouting:\n",
        "    hc = top_keywords_score(df, HEALTHCARE_KEYWORDS)\n",
        "    ai = top_keywords_score(df, AI_KEYWORDS)\n",
        "\n",
        "    cols = [str(c).lower() for c in df.columns]\n",
        "    has_patient_id = any(re.search(r\"\\b(mrn|patient_id|patientid|uhid)\\b\", c) for c in cols)\n",
        "    has_icd = any(\"icd\" in c for c in cols)\n",
        "    has_prompt = any(\"prompt\" in c for c in cols)\n",
        "    has_label = any(re.search(r\"\\b(label|target|class|y)\\b\", c) for c in cols)\n",
        "\n",
        "    hc += 2 if has_patient_id else 0\n",
        "    hc += 2 if has_icd else 0\n",
        "    ai += 2 if has_prompt else 0\n",
        "    ai += 1 if has_label else 0\n",
        "\n",
        "    total = hc + ai + 2\n",
        "    p_hc = (hc + 1) / total\n",
        "    p_ai = (ai + 1) / total\n",
        "    chosen = \"Healthcare\" if p_hc >= p_ai else \"AI Company\"\n",
        "\n",
        "    return DomainRouting(\n",
        "        p_healthcare=float(p_hc),\n",
        "        p_ai=float(p_ai),\n",
        "        chosen=chosen,\n",
        "        evidence={\n",
        "            \"keyword_hits\": {\"healthcare\": hc, \"ai\": ai},\n",
        "            \"structural_hints\": {\"has_patient_id\": has_patient_id, \"has_icd\": has_icd, \"has_prompt\": has_prompt, \"has_label\": has_label},\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 5) Forensic statistics core\n",
        "# =========================================================\n",
        "def duplicate_rows(df: pd.DataFrame) -> Finding:\n",
        "    total = len(df)\n",
        "    dup = int(df.duplicated().sum())\n",
        "    rate = dup / total if total else 0.0\n",
        "    sev = severity_from_threshold(rate, THRESH[\"dup_row_pct_warn\"], THRESH[\"dup_row_pct_fail\"], True)\n",
        "    return Finding(\"Duplicate rows\", sev, {\"duplicate_rows\": dup, \"rows\": int(total), \"duplicate_rate\": float(rate)},\n",
        "                   \"High duplication can indicate templating/copying, synthetic repetition, or pipeline errors.\")\n",
        "\n",
        "def missingness(df: pd.DataFrame) -> Finding:\n",
        "    overall = float(df.isna().mean().mean())\n",
        "    top = df.isna().mean().sort_values(ascending=False).head(12)\n",
        "    sev = severity_from_threshold(overall, THRESH[\"missing_overall_warn\"], THRESH[\"missing_overall_fail\"], True)\n",
        "    return Finding(\"Missingness profile\", sev,\n",
        "                   {\"overall_missing_rate\": overall, \"top_missing\": [(k, float(v)) for k, v in top.items()]},\n",
        "                   \"Excess missingness can hide manipulation or indicate broken collection pipelines.\")\n",
        "\n",
        "def benford_first_digit(series: pd.Series) -> Dict[str, Any]:\n",
        "    s = to_numeric_clean(series)\n",
        "    s = s[s != 0].abs()\n",
        "    if len(s) < 250:\n",
        "        return {\"applicable\": False, \"reason\": \"Too few numeric values (n<250).\", \"n\": int(len(s))}\n",
        "    first = s.astype(str).str.replace(r\"[^0-9.]\", \"\", regex=True)\n",
        "    first = first.str.lstrip(\"0\").str.replace(\".\", \"\", regex=False)\n",
        "    first = first[first.str.len() > 0].str[0]\n",
        "    first = pd.to_numeric(first, errors=\"coerce\").dropna().astype(int)\n",
        "    first = first[first.between(1, 9)]\n",
        "    if len(first) < 250:\n",
        "        return {\"applicable\": False, \"reason\": \"Too few usable first digits.\", \"n\": int(len(first))}\n",
        "    counts = np.array([(first == d).sum() for d in range(1, 10)], dtype=float)\n",
        "    exp = np.array([math.log10(1 + 1/d) for d in range(1, 10)], dtype=float) * counts.sum()\n",
        "    chi, p = chisquare(counts, f_exp=exp)\n",
        "    return {\"applicable\": True, \"n\": int(counts.sum()), \"chi2\": float(chi), \"p_value\": float(p)}\n",
        "\n",
        "def benford(df: pd.DataFrame) -> Finding:\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if not num_cols:\n",
        "        return Finding(\"Benford test\", \"INFO\", {\"applicable\": False, \"reason\": \"No numeric columns.\"},\n",
        "                       \"Benford requires naturally occurring numbers; not applicable here.\")\n",
        "    col = num_cols[0]\n",
        "    res = benford_first_digit(df[col])\n",
        "    if not res.get(\"applicable\"):\n",
        "        return Finding(f\"Benford test on '{col}'\", \"INFO\", res, \"Benford not applicable or insufficient data.\")\n",
        "    p = res[\"p_value\"]\n",
        "    sev = \"FAIL\" if p <= THRESH[\"benford_p_fail\"] else (\"WARN\" if p <= THRESH[\"benford_p_warn\"] else \"INFO\")\n",
        "    return Finding(f\"Benford test on '{col}'\", sev, res,\n",
        "                   \"Low p suggests deviation (can be benign due to bounded scales; may also suggest fabrication).\")\n",
        "\n",
        "def rounding(series: pd.Series) -> Dict[str, Any]:\n",
        "    s = to_numeric_clean(series)\n",
        "    if len(s) < 200:\n",
        "        return {\"applicable\": False, \"n\": int(len(s))}\n",
        "    frac = np.abs(s - np.round(s))\n",
        "    pct_int = float((frac < 1e-12).mean())\n",
        "    pct_1dp = float((np.abs(s * 10 - np.round(s * 10)) < 1e-12).mean())\n",
        "    pct_2dp = float((np.abs(s * 100 - np.round(s * 100)) < 1e-12).mean())\n",
        "    return {\"applicable\": True, \"n\": int(len(s)), \"pct_integer\": pct_int, \"pct_1dp\": pct_1dp, \"pct_2dp\": pct_2dp}\n",
        "\n",
        "def rounding_f(df: pd.DataFrame) -> Finding:\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if not num_cols:\n",
        "        return Finding(\"Rounding artifacts\", \"INFO\", {\"applicable\": False, \"reason\": \"No numeric columns.\"},\n",
        "                       \"Rounding checks need numeric columns.\")\n",
        "    col = num_cols[0]\n",
        "    res = rounding(df[col])\n",
        "    if not res.get(\"applicable\"):\n",
        "        return Finding(f\"Rounding artifacts on '{col}'\", \"INFO\", res, \"Not enough numeric values.\")\n",
        "    sev = \"FAIL\" if res[\"pct_integer\"] >= THRESH[\"rounding_integer_fail\"] else (\"WARN\" if res[\"pct_integer\"] >= THRESH[\"rounding_integer_warn\"] else \"INFO\")\n",
        "    return Finding(f\"Rounding artifacts on '{col}'\", sev, res,\n",
        "                   \"Extremely high rounding may indicate templating/synthetic generation (or true instrument precision).\")\n",
        "\n",
        "def entropy_f(df: pd.DataFrame) -> Finding:\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if num_cols:\n",
        "        col = num_cols[0]\n",
        "        s = to_numeric_clean(df[col])\n",
        "        if len(s) < 200:\n",
        "            return Finding(\"Entropy profile\", \"INFO\", {\"applicable\": False, \"n\": int(len(s))}, \"Too few values.\")\n",
        "        bins = np.histogram_bin_edges(s, bins=\"auto\")\n",
        "        hist, _ = np.histogram(s, bins=bins)\n",
        "        p = hist / max(hist.sum(), 1)\n",
        "        h = float(scipy_entropy(p + 1e-12))\n",
        "        sev = severity_from_threshold(h, THRESH[\"entropy_low_warn\"], THRESH[\"entropy_low_fail\"], higher_is_worse=False)\n",
        "        return Finding(f\"Entropy on '{col}' (binned)\", sev, {\"entropy\": h, \"bins\": int(len(hist)), \"n\": int(len(s))},\n",
        "                       \"Very low entropy can indicate over-smoothing/mode collapse; interpret with context.\")\n",
        "    obj_cols = df.select_dtypes(include=[\"object\", \"string\"]).columns.tolist()\n",
        "    if obj_cols:\n",
        "        col = obj_cols[0]\n",
        "        s = df[col].dropna().astype(str)\n",
        "        if len(s) < 200:\n",
        "            return Finding(\"Entropy profile\", \"INFO\", {\"applicable\": False, \"n\": int(len(s))}, \"Too few values.\")\n",
        "        vc = s.value_counts().head(300)\n",
        "        p = vc.values / vc.values.sum()\n",
        "        h = float(scipy_entropy(p + 1e-12))\n",
        "        return Finding(f\"Entropy on '{col}' (categorical)\", \"INFO\", {\"entropy\": h, \"k\": int(len(vc)), \"n\": int(len(s))},\n",
        "                       \"Categorical entropy is contextual.\")\n",
        "    return Finding(\"Entropy profile\", \"INFO\", {\"applicable\": False}, \"No usable columns for entropy.\")\n",
        "\n",
        "def corr_anoms(df: pd.DataFrame) -> Finding:\n",
        "    num = df.select_dtypes(include=[np.number])\n",
        "    if num.shape[1] < 3:\n",
        "        return Finding(\"Correlation structure\", \"INFO\", {\"applicable\": False, \"reason\": \"Too few numeric cols.\"},\n",
        "                       \"Need >=3 numeric cols.\")\n",
        "    corr = num.corr().abs()\n",
        "    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
        "    high = upper.stack()\n",
        "    high = high[high >= 0.98].sort_values(ascending=False)\n",
        "    pairs = [(a, b, float(v)) for (a, b), v in high.head(30).items()]\n",
        "    n = len(pairs)\n",
        "    sev = \"FAIL\" if n >= THRESH[\"high_corr_fail\"] else (\"WARN\" if n >= THRESH[\"high_corr_warn\"] else \"INFO\")\n",
        "    return Finding(\"Near-perfect correlations (>=0.98)\", sev, {\"count\": n, \"pairs_top\": pairs[:15]},\n",
        "                   \"Excess near-perfect correlations can indicate leakage/derived cols or synthetic construction.\")\n",
        "\n",
        "def split_shift(df: pd.DataFrame) -> Finding:\n",
        "    if len(df) < 600:\n",
        "        return Finding(\"Split-half KS shift\", \"INFO\", {\"applicable\": False, \"reason\": \"Too few rows.\"},\n",
        "                       \"Need >=600 rows.\")\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    if len(num_cols) < 2:\n",
        "        return Finding(\"Split-half KS shift\", \"INFO\", {\"applicable\": False, \"reason\": \"Too few numeric cols.\"},\n",
        "                       \"Need >=2 numeric cols.\")\n",
        "    mid = len(df)//2\n",
        "    a, b = df.iloc[:mid], df.iloc[mid:]\n",
        "    shifts=[]\n",
        "    for c in num_cols[:20]:\n",
        "        sa, sb = to_numeric_clean(a[c]), to_numeric_clean(b[c])\n",
        "        if len(sa) < 150 or len(sb) < 150:\n",
        "            continue\n",
        "        stat, p = ks_2samp(sa.values, sb.values)\n",
        "        shifts.append((c, float(stat), float(p)))\n",
        "    low_p = [x for x in shifts if x[2] < 0.01]\n",
        "    rate = len(low_p)/max(1, len(shifts))\n",
        "    sev = \"FAIL\" if rate >= 0.50 else (\"WARN\" if rate >= 0.25 else \"INFO\")\n",
        "    return Finding(\"Split-half distribution shift (KS)\", sev,\n",
        "                   {\"tested_cols\": len(shifts), \"low_p_cols\": len(low_p), \"rate\": float(rate), \"examples\": low_p[:10]},\n",
        "                   \"Widespread shifts can indicate stitching/backfill/regime changes.\")\n",
        "\n",
        "def moments(df: pd.DataFrame) -> Finding:\n",
        "    num = df.select_dtypes(include=[np.number])\n",
        "    if num.shape[1] < 2:\n",
        "        return Finding(\"Moment fingerprints\", \"INFO\", {\"applicable\": False, \"reason\": \"Too few numeric cols.\"},\n",
        "                       \"Need >=2 numeric cols.\")\n",
        "    stats=[]\n",
        "    for c in num.columns[:25]:\n",
        "        s = to_numeric_clean(num[c])\n",
        "        if len(s) < 200:\n",
        "            continue\n",
        "        stats.append((c, float(skew(s)), float(kurtosis(s, fisher=True))))\n",
        "    if not stats:\n",
        "        return Finding(\"Moment fingerprints\", \"INFO\", {\"applicable\": False, \"reason\": \"Not enough data per col.\"},\n",
        "                       \"Not enough data.\")\n",
        "    near0 = sum(1 for _,_,k in stats if abs(k) < 0.25)\n",
        "    rate = near0/len(stats)\n",
        "    sev = \"WARN\" if rate > 0.65 else \"INFO\"\n",
        "    return Finding(\"Moment fingerprints (skew/kurtosis)\", sev,\n",
        "                   {\"cols\": len(stats), \"near_zero_kurtosis_rate\": float(rate), \"examples\": stats[:10]},\n",
        "                   \"Many near-zero kurtosis columns can indicate over-smoothing; interpret with context.\")\n",
        "\n",
        "def forensic_core(df: pd.DataFrame) -> List[Finding]:\n",
        "    return [duplicate_rows(df), missingness(df), benford(df), rounding_f(df), entropy_f(df),\n",
        "            corr_anoms(df), split_shift(df), moments(df)]\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 6) Formal invariants\n",
        "# =========================================================\n",
        "def inv_unique_columns(df: pd.DataFrame) -> Invariant:\n",
        "    ok = len(set(df.columns)) == len(df.columns)\n",
        "    return Invariant(\"Schema: unique column names\", \"PASS\" if ok else \"FAIL\",\n",
        "                     {\"unique\": ok}, \"Duplicate columns cause ambiguity and can mask tampering.\")\n",
        "\n",
        "def inv_parseable_time(df: pd.DataFrame) -> Invariant:\n",
        "    time_cols = [c for c in df.columns if any(k in str(c).lower() for k in (\"time\",\"timestamp\",\"date\"))]\n",
        "    if not time_cols:\n",
        "        return Invariant(\"Temporal: timestamp parseability\", \"UNKNOWN\",\n",
        "                         {\"reason\":\"No timestamp-like columns\"}, \"Needs a timestamp column.\")\n",
        "    c = time_cols[0]\n",
        "    parsed = pd.to_datetime(df[c], errors=\"coerce\", utc=True)\n",
        "    rate = float(parsed.notna().mean())\n",
        "    status = \"PASS\" if rate >= 0.90 else (\"FAIL\" if rate < 0.75 else \"UNKNOWN\")\n",
        "    return Invariant(f\"Temporal: '{c}' parseability\", status, {\"parseable_rate\": rate, \"column\": c},\n",
        "                     \"Low parseability indicates inconsistent timestamps or schema misuse.\")\n",
        "\n",
        "def inv_id_dup(df: pd.DataFrame) -> Invariant:\n",
        "    id_cols = [c for c in df.columns if re.search(r\"\\b(id|uuid|mrn|patient_id|record_id)\\b\", str(c).lower())]\n",
        "    if not id_cols:\n",
        "        return Invariant(\"Identity: ID duplication sanity\", \"UNKNOWN\",\n",
        "                         {\"reason\":\"No ID-like columns\"}, \"Needs an ID column.\")\n",
        "    c = id_cols[0]\n",
        "    s = df[c].dropna().astype(str)\n",
        "    if len(s) < 100:\n",
        "        return Invariant(f\"Identity: '{c}' duplication sanity\", \"UNKNOWN\",\n",
        "                         {\"reason\":\"Too few IDs\", \"column\": c}, \"Not enough IDs.\")\n",
        "    dup = float(s.duplicated().mean())\n",
        "    status = \"PASS\" if dup <= 0.05 else (\"FAIL\" if dup >= 0.20 else \"UNKNOWN\")\n",
        "    return Invariant(f\"Identity: '{c}' duplication rate\", status, {\"dup_rate\": dup, \"column\": c},\n",
        "                     \"Very high duplication suggests broken entity identity or fabricated records.\")\n",
        "\n",
        "def inv_time_order(df: pd.DataFrame) -> Invariant:\n",
        "    lower = {str(c).lower(): c for c in df.columns}\n",
        "    start_keys = [\"start\",\"admission\",\"admit\",\"begin\"]\n",
        "    end_keys = [\"end\",\"discharge\",\"release\",\"finish\"]\n",
        "    start_col = next((lower[k] for k in lower if any(s in k for s in start_keys)), None)\n",
        "    end_col = next((lower[k] for k in lower if any(e in k for e in end_keys)), None)\n",
        "    if not start_col or not end_col:\n",
        "        return Invariant(\"Temporal: no time-travel ordering\", \"UNKNOWN\",\n",
        "                         {\"reason\":\"No obvious start/end pair\"}, \"Needs start/end timestamps.\")\n",
        "    a = pd.to_datetime(df[start_col], errors=\"coerce\", utc=True)\n",
        "    b = pd.to_datetime(df[end_col], errors=\"coerce\", utc=True)\n",
        "    m = a.notna() & b.notna()\n",
        "    if m.mean() < 0.30:\n",
        "        return Invariant(\"Temporal: no time-travel ordering\", \"UNKNOWN\",\n",
        "                         {\"reason\":\"Too few paired timestamps\"}, \"Not enough pairs.\")\n",
        "    violations = int((b[m] < a[m]).sum())\n",
        "    rate = float(violations / max(1, int(m.sum())))\n",
        "    status = \"PASS\" if rate == 0.0 else (\"FAIL\" if rate > 0.02 else \"UNKNOWN\")\n",
        "    return Invariant(f\"Temporal ordering: '{end_col}' >= '{start_col}'\", status,\n",
        "                     {\"violations\": violations, \"paired\": int(m.sum()), \"violation_rate\": rate},\n",
        "                     \"End-before-start violations can indicate tampering or merge errors.\")\n",
        "\n",
        "def invariants(df: pd.DataFrame) -> List[Invariant]:\n",
        "    return [inv_unique_columns(df), inv_parseable_time(df), inv_id_dup(df), inv_time_order(df)]\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 7) AI integrity engine (embedding + anomaly + disagreement)\n",
        "# =========================================================\n",
        "def build_embedding_pipe(df: pd.DataFrame) -> Pipeline:\n",
        "    num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = [c for c in df.columns if c not in num_cols]\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", Pipeline([(\"scaler\", StandardScaler(with_mean=False))]), num_cols),\n",
        "            (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", min_frequency=5), cat_cols),\n",
        "        ],\n",
        "        remainder=\"drop\",\n",
        "        sparse_threshold=0.3,\n",
        "    )\n",
        "    return Pipeline([(\"pre\", pre), (\"svd\", TruncatedSVD(n_components=32, random_state=42))])\n",
        "\n",
        "def ai_engine(df: pd.DataFrame) -> AISignals:\n",
        "    if len(df) < 800:\n",
        "        return AISignals(False, 0.0, 1.0, [\"Not enough rows for AI model (n<800).\"], {})\n",
        "    try:\n",
        "        dfx = df.copy()\n",
        "        if len(dfx) > 60_000:\n",
        "            dfx = dfx.sample(n=60_000, random_state=42)\n",
        "        pipe = build_embedding_pipe(dfx)\n",
        "        Z = pipe.fit_transform(dfx)\n",
        "\n",
        "        iso = IsolationForest(n_estimators=200, contamination=\"auto\", random_state=42, n_jobs=-1)\n",
        "        pred = iso.fit_predict(Z)\n",
        "        out_rate = float((pred == -1).mean())\n",
        "\n",
        "        idx = np.arange(Z.shape[0])\n",
        "        np.random.shuffle(idx)\n",
        "        a = Z[idx[:len(idx)//2]]\n",
        "        b = Z[idx[len(idx)//2:]]\n",
        "        ca = a.mean(axis=0, keepdims=True)\n",
        "        cb = b.mean(axis=0, keepdims=True)\n",
        "        cos = float(cosine_similarity(ca, cb)[0,0])\n",
        "\n",
        "        notes=[]\n",
        "        sev_out = severity_from_threshold(out_rate, THRESH[\"isoforest_outlier_warn\"], THRESH[\"isoforest_outlier_fail\"], True)\n",
        "        if sev_out != \"INFO\":\n",
        "            notes.append(f\"Outlier rate elevated: {out_rate:.2%} (IsolationForest in latent space).\")\n",
        "        if cos < 0.985:\n",
        "            notes.append(f\"Centroid disagreement across splits: cosine={cos:.4f} (mixing/instability).\")\n",
        "\n",
        "        return AISignals(True, out_rate, cos, notes, {\"used_rows\": int(len(dfx)), \"latent_dim\": int(Z.shape[1])})\n",
        "    except Exception as e:\n",
        "        return AISignals(False, 0.0, 1.0, [f\"AI engine error: {e}\"], {})\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 8) Industry packs\n",
        "# =========================================================\n",
        "def healthcare_pack(df: pd.DataFrame) -> List[Finding]:\n",
        "    cols = {str(c).lower(): c for c in df.columns}\n",
        "    out=[]\n",
        "    def bounds(key, lo, hi, label):\n",
        "        if key in cols:\n",
        "            c = cols[key]\n",
        "            s = to_numeric_clean(df[c])\n",
        "            if len(s) < 200: return\n",
        "            bad = int(((s < lo) | (s > hi)).sum())\n",
        "            rate = bad / max(1, len(s))\n",
        "            sev = \"FAIL\" if rate > 0.05 else (\"WARN\" if rate > 0.01 else \"INFO\")\n",
        "            out.append(Finding(f\"Healthcare bounds: {c}\", sev,\n",
        "                              {\"column\": c, \"bounds\":[lo,hi], \"violations\": bad, \"n\": int(len(s)), \"violation_rate\": float(rate)},\n",
        "                              f\"{label} plausibility bounds. Violations can indicate unit errors/corruption/fabrication.\"))\n",
        "    bounds(\"age\", 0, 120, \"Age\")\n",
        "    bounds(\"spo2\", 50, 100, \"SpOâ‚‚\")\n",
        "    bounds(\"hr\", 20, 250, \"Heart rate\")\n",
        "    bounds(\"heart_rate\", 20, 250, \"Heart rate\")\n",
        "    bounds(\"systolic\", 50, 250, \"Systolic BP\")\n",
        "    bounds(\"diastolic\", 20, 150, \"Diastolic BP\")\n",
        "    bounds(\"temp\", 30, 45, \"Temperature\")\n",
        "    bounds(\"temperature\", 30, 45, \"Temperature\")\n",
        "    return out\n",
        "\n",
        "def ai_pack(df: pd.DataFrame) -> List[Finding]:\n",
        "    out=[]\n",
        "    label_cols = [c for c in df.columns if str(c).lower() in (\"label\",\"target\",\"y\",\"class\")]\n",
        "    if label_cols:\n",
        "        c = label_cols[0]\n",
        "        vc = df[c].value_counts(dropna=False)\n",
        "        total = int(vc.sum())\n",
        "        top_pct = float(vc.iloc[0]/total) if total else 0.0\n",
        "        sev = \"WARN\" if top_pct > 0.90 else \"INFO\"\n",
        "        out.append(Finding(f\"AI pack: label imbalance '{c}'\", sev,\n",
        "                           {\"column\": c, \"top_class_pct\": top_pct, \"top_classes\": vc.head(10).to_dict()},\n",
        "                           \"Extreme imbalance may indicate bias, leakage, or synthetic oversampling.\"))\n",
        "    text_cols = [c for c in df.columns if any(k in str(c).lower() for k in (\"text\",\"prompt\",\"query\",\"content\"))]\n",
        "    if text_cols:\n",
        "        c = text_cols[0]\n",
        "        s = df[c].dropna().astype(str)\n",
        "        if len(s)>0:\n",
        "            dup = int(s.duplicated().sum())\n",
        "            rate = float(dup/len(s))\n",
        "            sev = \"FAIL\" if rate > 0.25 else (\"WARN\" if rate > 0.08 else \"INFO\")\n",
        "            out.append(Finding(f\"AI pack: text duplication '{c}'\", sev,\n",
        "                               {\"column\": c, \"duplicate_count\": dup, \"n\": int(len(s)), \"duplicate_rate\": rate},\n",
        "                               \"High duplication suggests templating/scraping artifacts or synthetic regeneration.\"))\n",
        "    return out\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 9) Reality Stress Testing (collapse curve + RRI)\n",
        "# =========================================================\n",
        "def reality_stress(df: pd.DataFrame) -> RealityStress:\n",
        "    num = df.select_dtypes(include=[np.number])\n",
        "    if num.shape[1] < 3 or len(df) < 1500:\n",
        "        return RealityStress(False, 0.70, [], [\"Insufficient numeric structure for v1 stress testing.\"],\n",
        "                            [\"RST not applicable; using default RRI=0.70.\"], {})\n",
        "\n",
        "    dfx = df\n",
        "    if len(dfx) > 80_000:\n",
        "        dfx = dfx.sample(n=80_000, random_state=42)\n",
        "    num = dfx.select_dtypes(include=[np.number]).copy()\n",
        "    cols = list(num.columns[:12])\n",
        "    base = num[cols]\n",
        "\n",
        "    def ks_fail_rate(test_df: pd.DataFrame, p_thresh: float = 0.01) -> float:\n",
        "        fails=0; tested=0\n",
        "        for c in cols:\n",
        "            a = to_numeric_clean(base[c])\n",
        "            b = to_numeric_clean(test_df[c])\n",
        "            if len(a)<500 or len(b)<500: continue\n",
        "            _, p = ks_2samp(a.values, b.values)\n",
        "            tested += 1\n",
        "            if p < p_thresh: fails += 1\n",
        "        return fails / max(1, tested)\n",
        "\n",
        "    curve=[]\n",
        "    for s in [0.0, 0.1, 0.2, 0.35, 0.5]:\n",
        "        test = num[cols].copy()\n",
        "        if s>0:\n",
        "            frac = max(0.5, 1.0 - s)\n",
        "            test = test.sample(frac=frac, random_state=int(s*1000))\n",
        "            for c in cols:\n",
        "                col = to_numeric_clean(test[c])\n",
        "                if len(col) < 500: continue\n",
        "                sd = float(np.nanstd(col.values))\n",
        "                if sd <= 0: continue\n",
        "                noise = np.random.normal(0.0, sd*(s*0.15), size=len(test))\n",
        "                test.loc[test.index, c] = pd.to_numeric(test[c], errors=\"coerce\").values + noise\n",
        "        fr = ks_fail_rate(test, 0.01)\n",
        "        curve.append((float(s), float(fr)))\n",
        "\n",
        "    xs = np.array([x for x,_ in curve]); ys = np.array([y for _,y in curve])\n",
        "    auc = float(np.trapz(ys, xs) / max(1e-9, xs.max()))\n",
        "    rri = float(np.clip(1.0 - auc, 0.0, 1.0))\n",
        "\n",
        "    notes = [\"Reality robustness high under v1 stress tests.\" if rri>=0.75 else\n",
        "             \"Reality robustness moderate; review collapse modes.\" if rri>=0.60 else\n",
        "             \"Reality robustness low; fingerprints collapse quickly under perturbation.\"]\n",
        "\n",
        "    return RealityStress(True, rri, curve,\n",
        "                        [\"Distribution stability under subsampling\", \"Noise-floor realism under perturbation\", \"Multi-column coherence under stress (KS)\"],\n",
        "                        notes, {\"cols_used\": cols, \"used_rows\": int(len(dfx))})\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 10) Scoring + verdict\n",
        "# =========================================================\n",
        "def score_findings(fs: List[Finding]) -> float:\n",
        "    score=100.0\n",
        "    for f in fs:\n",
        "        if f.severity==\"WARN\": score -= 8\n",
        "        elif f.severity==\"FAIL\": score -= 18\n",
        "    return float(np.clip(score,0,100))\n",
        "\n",
        "def score_invariants(inv: List[Invariant]) -> float:\n",
        "    score=100.0\n",
        "    for x in inv:\n",
        "        if x.status==\"FAIL\": score -= 20\n",
        "        elif x.status==\"UNKNOWN\": score -= 6\n",
        "    return float(np.clip(score,0,100))\n",
        "\n",
        "def score_ai(ai: AISignals) -> float:\n",
        "    if not ai.applicable:\n",
        "        return 75.0\n",
        "    sc = 100.0\n",
        "    sc -= 60.0 * float(np.clip(ai.outlier_rate, 0, 0.25))\n",
        "    if ai.centroid_cosine < 0.985:\n",
        "        sc -= 12.0\n",
        "    return float(np.clip(sc,0,100))\n",
        "\n",
        "def verdict(scores: Dict[str,float]) -> str:\n",
        "    if scores[\"statistical_integrity\"] < 60 or scores[\"invariant_integrity\"] < 60 or scores[\"reality_robustness\"] < 55:\n",
        "        return \"COMPROMISED / DO NOT USE (high risk)\"\n",
        "    if scores[\"statistical_integrity\"] < 75 or scores[\"invariant_integrity\"] < 75 or scores[\"reality_robustness\"] < 70:\n",
        "        return \"CONDITIONAL (use only with controls + provenance)\"\n",
        "    return \"ACCEPTABLE (MVP checks passed; enable provenance for high-stakes use)\"\n",
        "\n",
        "def recommendations(scores: Dict[str,float], routing: DomainRouting) -> List[str]:\n",
        "    rec=[]\n",
        "    if scores[\"invariant_integrity\"] < 80: rec.append(\"Fix schema/ID/timestamp invariant issues before relying on this dataset.\")\n",
        "    if scores[\"statistical_integrity\"] < 80: rec.append(\"Investigate forensic flags (missingness/duplication/Benford/rounding/correlation) and rerun.\")\n",
        "    if scores[\"reality_robustness\"] < 75: rec.append(\"Dataset is brittle under stress tests; request collection/process documentation and run deeper checks.\")\n",
        "    rec.append(\"Next step: enable Proof-of-Provenance (signed receipts + version diffs) to prove what existed when and detect tampering.\")\n",
        "    rec.append(\"Healthcare: validate units and enforce physiologic constraints upstream.\" if routing.chosen==\"Healthcare\"\n",
        "               else \"AI companies: lock eval sets by hash and run leakage/duplication controls.\")\n",
        "    return rec\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 11) PDF report\n",
        "# =========================================================\n",
        "def generate_pdf(report: Dict[str, Any], out_path: str = \"resethiq_integrity_report.pdf\") -> str:\n",
        "    c = canvas.Canvas(out_path, pagesize=A4)\n",
        "    w,h = A4\n",
        "\n",
        "    def header():\n",
        "        c.setFont(\"Helvetica-Bold\", 16)\n",
        "        c.setFillColor(colors.black)\n",
        "        c.drawString(2*cm, h-2*cm, \"Resethiq Integrity Report (Colab MVP)\")\n",
        "        c.setFont(\"Helvetica\", 9)\n",
        "        c.setFillColor(colors.grey)\n",
        "        meta = report[\"meta\"]\n",
        "        c.drawString(2*cm, h-2.6*cm,\n",
        "                     f\"Generated: {meta['generated_at']} | Fingerprint: {meta['fingerprint'][:16]}â€¦ | Rows used: {meta['used_rows']}/{meta['rows']}\")\n",
        "        c.setStrokeColor(colors.lightgrey)\n",
        "        c.line(2*cm, h-2.85*cm, w-2*cm, h-2.85*cm)\n",
        "\n",
        "    def ensure(y, need=2*cm):\n",
        "        if y < need:\n",
        "            c.showPage()\n",
        "            header()\n",
        "            return h - 3.3*cm\n",
        "        return y\n",
        "\n",
        "    def section(y, title, lines):\n",
        "        y = ensure(y)\n",
        "        c.setFont(\"Helvetica-Bold\", 12)\n",
        "        c.setFillColor(colors.black)\n",
        "        c.drawString(2*cm, y, title)\n",
        "        y -= 0.6*cm\n",
        "        c.setFont(\"Helvetica\", 10)\n",
        "        for ln in lines:\n",
        "            y = ensure(y)\n",
        "            c.drawString(2.2*cm, y, str(ln)[:115])\n",
        "            y -= 0.45*cm\n",
        "        return y - 0.3*cm\n",
        "\n",
        "    header()\n",
        "    y = h - 3.5*cm\n",
        "\n",
        "    scores = report[\"scores\"]\n",
        "    routing = report[\"routing\"]\n",
        "    y = section(y, \"1) Executive Summary\", [\n",
        "        f\"Industry routing: {routing['chosen']} (p_healthcare={routing['p_healthcare']:.2f}, p_ai={routing['p_ai']:.2f})\",\n",
        "        f\"Verdict: {report['verdict']}\",\n",
        "        f\"Statistical integrity: {scores['statistical_integrity']:.1f}/100\",\n",
        "        f\"Invariant integrity: {scores['invariant_integrity']:.1f}/100\",\n",
        "        f\"AI integrity: {scores['ai_integrity']:.1f}/100\",\n",
        "        f\"Reality robustness: {scores['reality_robustness']:.1f}/100\",\n",
        "        f\"CSV parse: {report.get('read_meta',{})}\",\n",
        "    ])\n",
        "\n",
        "    y = section(y, \"2) Forensic Findings (Core)\", [\n",
        "        f\"[{f['severity']}] {f['name']} | {json.dumps(f['metric'])[:90]}...\" for f in report[\"forensic_findings\"][:24]\n",
        "    ] or [\"No forensic findings.\"])\n",
        "\n",
        "    y = section(y, \"3) Formal Invariants\", [\n",
        "        f\"[{i['status']}] {i['name']} | {json.dumps(i['details'])[:90]}...\" for i in report[\"invariants\"][:24]\n",
        "    ] or [\"No invariants.\"])\n",
        "\n",
        "    ai = report[\"ai_signals\"]\n",
        "    ai_lines = [\n",
        "        f\"Applicable: {ai['applicable']}\",\n",
        "        f\"Outlier rate: {ai.get('outlier_rate',0):.2%}\",\n",
        "        f\"Centroid cosine: {ai.get('centroid_cosine',1.0):.4f}\",\n",
        "    ] + [f\"- {n}\" for n in ai.get(\"notes\",[])[:10]]\n",
        "    y = section(y, \"4) AI Integrity Signals\", ai_lines)\n",
        "\n",
        "    rst = report[\"reality\"]\n",
        "    rst_lines = [f\"Applicable: {rst['applicable']}\", f\"RRI: {rst['rri']:.3f}\"]\n",
        "    if rst.get(\"collapse_curve\"):\n",
        "        rst_lines.append(\"Collapse curve (stress -> KS fail-rate):\")\n",
        "        rst_lines += [f\"  â€¢ stress={x:.2f} -> fail_rate={fr:.2f}\" for x,fr in rst[\"collapse_curve\"]]\n",
        "    rst_lines += [f\"- {n}\" for n in rst.get(\"notes\",[])[:8]]\n",
        "    y = section(y, \"5) Reality Stress Testing (CDRM-lite)\", rst_lines)\n",
        "\n",
        "    y = section(y, \"6) Industry Pack Findings\", [\n",
        "        f\"[{f['severity']}] {f['name']} | {json.dumps(f['metric'])[:90]}...\" for f in report[\"industry_pack_findings\"][:18]\n",
        "    ] or [\"No industry pack findings.\"])\n",
        "\n",
        "    y = section(y, \"7) Recommendations\", [f\"- {r}\" for r in report[\"recommendations\"][:14]])\n",
        "\n",
        "    c.showPage()\n",
        "    c.save()\n",
        "    return out_path\n",
        "\n",
        "\n",
        "# =========================================================\n",
        "# 12) Orchestrator (read -> run -> report)\n",
        "# =========================================================\n",
        "def run_resethiq_mvp_from_path(path: str) -> Dict[str, Any]:\n",
        "    df_raw, read_meta = load_csv_robust(path)\n",
        "    df, samp_meta = safe_sample(df_raw)\n",
        "\n",
        "    meta = {\n",
        "        \"generated_at\": datetime.utcnow().strftime(\"%Y-%m-%d %H:%M UTC\"),\n",
        "        \"fingerprint\": stable_dataset_fingerprint(df),\n",
        "        \"rows\": int(len(df_raw)),\n",
        "        \"cols\": int(df_raw.shape[1]),\n",
        "        \"used_rows\": int(len(df)),\n",
        "        \"sampled\": bool(samp_meta[\"sampled\"]),\n",
        "    }\n",
        "\n",
        "    routing = domain_router(df)\n",
        "    core = forensic_core(df)\n",
        "    invs = invariants(df)\n",
        "    ai = ai_engine(df)\n",
        "    rst = reality_stress(df)\n",
        "\n",
        "    pack_findings = healthcare_pack(df) if routing.chosen == \"Healthcare\" else ai_pack(df)\n",
        "\n",
        "    scores = {\n",
        "        \"statistical_integrity\": score_findings(core + pack_findings),\n",
        "        \"invariant_integrity\": score_invariants(invs),\n",
        "        \"ai_integrity\": score_ai(ai),\n",
        "        \"reality_robustness\": float(np.clip(100.0 * rst.rri, 0, 100)),\n",
        "    }\n",
        "\n",
        "    rep = {\n",
        "        \"meta\": meta,\n",
        "        \"read_meta\": read_meta,\n",
        "        \"routing\": asdict(routing),\n",
        "        \"forensic_findings\": [asdict(x) for x in core],\n",
        "        \"invariants\": [asdict(x) for x in invs],\n",
        "        \"ai_signals\": asdict(ai),\n",
        "        \"reality\": asdict(rst),\n",
        "        \"industry_pack_findings\": [asdict(x) for x in pack_findings],\n",
        "        \"scores\": scores,\n",
        "    }\n",
        "\n",
        "    rep[\"verdict\"] = verdict(scores)\n",
        "    rep[\"recommendations\"] = recommendations(scores, routing)\n",
        "    return rep\n"
      ],
      "metadata": {
        "id": "NE1R94u_oHZC"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report = run_resethiq_mvp_from_path(csv_path)\n",
        "report[\"verdict\"], report[\"scores\"], report[\"read_meta\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9wAMh09pSxm",
        "outputId": "6690abc7-ebbb-49dd-e648-0d24ae2beec1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3351063539.py:753: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  \"generated_at\": datetime.utcnow().strftime(\"%Y-%m-%d %H:%M UTC\"),\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('ACCEPTABLE (MVP checks passed; enable provenance for high-stakes use)',\n",
              " {'statistical_integrity': 100.0,\n",
              "  'invariant_integrity': 82.0,\n",
              "  'ai_integrity': 75.0,\n",
              "  'reality_robustness': 70.0},\n",
              " {'encoding': 'utf-16le', 'encoding_errors': 'replace', 'sep': ','})"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = generate_pdf(report, out_path=\"resethiq_integrity_report.pdf\")\n",
        "pdf_path\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "9nMxUwPNpUOS",
        "outputId": "0e20bc5a-4d27-4f92-e371-7bd17a48da3c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'resethiq_integrity_report.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(pdf_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "oDjmgN3EpXR3",
        "outputId": "f13cd9d1-3567-46ef-a014-f919d407093f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_9404a8e1-11fc-410d-8fae-a410f986cdbb\", \"resethiq_integrity_report.pdf\", 3354)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=== ROUTING ===\")\n",
        "print(report[\"routing\"])\n",
        "print(\"\\n=== VERDICT ===\")\n",
        "print(report[\"verdict\"])\n",
        "print(\"\\n=== SCORES ===\")\n",
        "print(report[\"scores\"])\n",
        "\n",
        "print(\"\\n=== FORENSIC FLAGS (WARN/FAIL) ===\")\n",
        "for f in report[\"forensic_findings\"]:\n",
        "    if f[\"severity\"] in (\"WARN\", \"FAIL\"):\n",
        "        print(f\"[{f['severity']}] {f['name']} -> {f['metric']}\")\n",
        "\n",
        "print(\"\\n=== INDUSTRY PACK FLAGS (WARN/FAIL) ===\")\n",
        "for f in report[\"industry_pack_findings\"]:\n",
        "    if f[\"severity\"] in (\"WARN\", \"FAIL\"):\n",
        "        print(f\"[{f['severity']}] {f['name']} -> {f['metric']}\")\n",
        "\n",
        "print(\"\\n=== INVARIANTS (FAIL/UNKNOWN) ===\")\n",
        "for inv in report[\"invariants\"]:\n",
        "    if inv[\"status\"] in (\"FAIL\", \"UNKNOWN\"):\n",
        "        print(f\"[{inv['status']}] {inv['name']} -> {inv['details']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1jUzXwsbpdio",
        "outputId": "7ca9d08c-2893-4ebc-bc6c-84b6ce348b77"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== ROUTING ===\n",
            "{'p_healthcare': 0.5, 'p_ai': 0.5, 'chosen': 'Healthcare', 'evidence': {'keyword_hits': {'healthcare': 0, 'ai': 0}, 'structural_hints': {'has_patient_id': False, 'has_icd': False, 'has_prompt': False, 'has_label': False}}}\n",
            "\n",
            "=== VERDICT ===\n",
            "ACCEPTABLE (MVP checks passed; enable provenance for high-stakes use)\n",
            "\n",
            "=== SCORES ===\n",
            "{'statistical_integrity': 100.0, 'invariant_integrity': 82.0, 'ai_integrity': 75.0, 'reality_robustness': 70.0}\n",
            "\n",
            "=== FORENSIC FLAGS (WARN/FAIL) ===\n",
            "\n",
            "=== INDUSTRY PACK FLAGS (WARN/FAIL) ===\n",
            "\n",
            "=== INVARIANTS (FAIL/UNKNOWN) ===\n",
            "[UNKNOWN] Temporal: timestamp parseability -> {'reason': 'No timestamp-like columns'}\n",
            "[UNKNOWN] Identity: ID duplication sanity -> {'reason': 'No ID-like columns'}\n",
            "[UNKNOWN] Temporal: no time-travel ordering -> {'reason': 'No obvious start/end pair'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(pdf_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "eW2aOSX3pikC",
        "outputId": "ba77a423-03d4-4eee-c440-875f2dfadf93"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7db01266-bd76-4db4-a9f7-dbebc0f1d33e\", \"resethiq_integrity_report.pdf\", 3354)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}